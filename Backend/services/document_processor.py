"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
 ğŸŒŒ COSMIC AI - Document Processing Pipeline
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import numpy as np
from typing import List
from services.document_parser import DocumentParser
from services.chunking import get_text_chunker
from services.embeddings import get_embedding_service
from services.vector_store import get_vector_store
from utils.logger import setup_logger

logger = setup_logger()


async def process_document(file_path: str, file_id: str):
    """
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  ğŸš€ Complete Document Processing Pipeline                   â”‚
    â”‚                                                             â”‚
    â”‚  1. Parse document â†’ Extract text                           â”‚
    â”‚  2. Chunk text â†’ Recursive splitting                        â”‚
    â”‚  3. Generate embeddings â†’ Azure OpenAI                      â”‚
    â”‚  4. Store in vector database â†’ FAISS HNSW                   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """
    
    try:
        filename = os.path.basename(file_path)
        
        logger.info("â•" * 60)
        logger.info(f"ğŸš€ PROCESSING DOCUMENT: {filename}")
        logger.info("â•" * 60)
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Step 1: Parse document
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        logger.info("\nğŸ“‹ STEP 1: Parsing Document")
        logger.info("â”€" * 40)
        
        parser = DocumentParser()
        text = parser.parse(file_path)
        
        if not text or not text.strip():
            logger.warning("âš ï¸ No text extracted from document")
            return
        
        logger.info(f"âœ… Extracted: {len(text)} characters")
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Step 2: Chunk text
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        logger.info("\nâœ‚ï¸ STEP 2: Chunking Text")
        logger.info("â”€" * 40)
        
        chunker = get_text_chunker()
        chunks = chunker.chunk_text(text)
        
        if not chunks:
            logger.warning("âš ï¸ No chunks generated")
            return
        
        logger.info(f"âœ… Generated: {len(chunks)} chunks")
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Step 3: Generate embeddings
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        logger.info("\nğŸ§  STEP 3: Generating Embeddings")
        logger.info("â”€" * 40)
        
        embedding_service = get_embedding_service()
        
        # Process in batches to avoid token limits
        BATCH_SIZE = 16  # Azure OpenAI batch limit
        all_embeddings = []
        
        total_batches = (len(chunks) - 1) // BATCH_SIZE + 1
        
        for i in range(0, len(chunks), BATCH_SIZE):
            batch = chunks[i:i+BATCH_SIZE]
            batch_num = i // BATCH_SIZE + 1
            
            logger.info(f"   â””â”€ Batch {batch_num}/{total_batches}: {len(batch)} chunks")
            
            batch_embeddings = embedding_service.embed_texts(batch)
            all_embeddings.append(batch_embeddings)
        
        # Concatenate all embeddings
        embeddings = np.vstack(all_embeddings)
        
        logger.info(f"âœ… Embeddings shape: {embeddings.shape}")
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Step 4: Store in vector database
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        logger.info("\nğŸ’¾ STEP 4: Storing in Vector Database")
        logger.info("â”€" * 40)
        
        vector_store = get_vector_store()
        
        vector_store.add_chunks(
            file_id=file_id,
            filename=filename,
            chunks=chunks,
            embeddings=embeddings
        )
        
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # Complete!
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        logger.info("\n" + "â•" * 60)
        logger.info(f"ğŸ‰ DOCUMENT PROCESSED SUCCESSFULLY!")
        logger.info(f"   â””â”€ File ID: {file_id}")
        logger.info(f"   â””â”€ Chunks: {len(chunks)}")
        logger.info(f"   â””â”€ Ready for RAG queries")
        logger.info("â•" * 60 + "\n")
        
    except Exception as e:
        logger.error(f"âŒ Error processing document {file_id}: {e}")
        
        # Mark as failed in vector store
        try:
            vector_store = get_vector_store()
            vector_store.mark_as_failed(file_id, str(e))
        except Exception as ve:
            logger.error(f"Failed to update vector store status: {ve}")
            
        raise
