# Vector Database Setup - FAISS with HNSW

## ðŸ“‹ Overview

This plan details the vector database implementation using **FAISS** (Facebook AI Similarity Search) with **HNSW** (Hierarchical Navigable Small World) indexing for fast approximate nearest neighbor search.

---

## ðŸŽ¯ Goals

1. Store document chunk embeddings efficiently
2. Use HNSW algorithm for sub-linear search time
3. Persist index to disk for durability
4. Support metadata filtering (by document ID)
5. Handle batch embedding operations
6. Provide easy retrieval of top-k similar chunks

---

## ðŸ”§ FAISS Overview

**FAISS** is a library developed by Meta AI Research for efficient similarity search and clustering of dense vectors. It's written in C++ with Python bindings.

### Why FAISS?
- âœ… Excellent performance for HNSW
- âœ… No external service needed (embedded)
- âœ… Battle-tested by Meta
- âœ… Easy Python integration
- âœ… CPU version requires no GPU

### HNSW in FAISS
- **Index Type**: `IndexHNSWFlat`
- **Search Complexity**: O(log N) average case
- **Memory**: Stores full vectors (no compression)
- **Accuracy**: High accuracy, tunable with `efSearch`

---

## ðŸ“ Vector Store Structure

```
vector_store/
â”œâ”€â”€ index.faiss          # FAISS HNSW index binary
â”œâ”€â”€ metadata.json        # Document metadata
â””â”€â”€ chunks.json          # Chunk text and file associations
```

### Data Schema

#### `metadata.json`
```json
{
  "documents": {
    "uuid-1234": {
      "filename": "document.pdf",
      "upload_date": "2025-12-27T10:00:00Z",
      "num_chunks": 15,
      "total_tokens": 5000
    }
  }
}
```

#### `chunks.json`
```json
{
  "chunks": [
    {
      "id": 0,
      "file_id": "uuid-1234",
      "content": "Chunk text content...",
      "chunk_index": 0,
      "tokens": 350
    }
  ]
}
```

---

## ðŸ› ï¸ Implementation

### `services/vector_store.py`

```python
import faiss
import numpy as np
import json
import os
from typing import List, Dict, Optional, Tuple
from datetime import datetime

from config.settings import settings
from utils.logger import setup_logger

logger = setup_logger()

class VectorStore:
    """FAISS-based vector store with HNSW indexing"""
    
    def __init__(self):
        self.dimension = 1536  # Azure OpenAI text-embedding-ada-002 dimension
        self.index: Optional[faiss.IndexHNSWFlat] = None
        self.chunks: List[Dict] = []
        self.metadata: Dict = {"documents": {}}
        
        self.index_path = os.path.join(settings.VECTOR_DB_PATH, "index.faiss")
        self.chunks_path = os.path.join(settings.VECTOR_DB_PATH, "chunks.json")
        self.metadata_path = os.path.join(settings.VECTOR_DB_PATH, "metadata.json")
        
        self._initialize_index()
    
    def _initialize_index(self):
        """Initialize or load FAISS HNSW index"""
        
        if os.path.exists(self.index_path):
            logger.info("Loading existing FAISS index...")
            self.index = faiss.read_index(self.index_path)
            self._load_chunks()
            self._load_metadata()
            logger.info(f"Loaded index with {self.index.ntotal} vectors")
        else:
            logger.info("Creating new FAISS HNSW index...")
            self.index = faiss.IndexHNSWFlat(
                self.dimension,
                settings.HNSW_M  # Number of connections per layer
            )
            # Set construction parameter
            self.index.hnsw.efConstruction = settings.HNSW_EF_CONSTRUCTION
            logger.info("New FAISS index created")
    
    def add_chunks(
        self,
        file_id: str,
        filename: str,
        chunks: List[str],
        embeddings: np.ndarray
    ):
        """Add document chunks and embeddings to the vector store"""
        
        if embeddings.shape[0] != len(chunks):
            raise ValueError("Number of embeddings must match number of chunks")
        
        if embeddings.shape[1] != self.dimension:
            raise ValueError(f"Embedding dimension must be {self.dimension}")
        
        # Convert to float32 if needed
        embeddings = embeddings.astype('float32')
        
        # Normalize embeddings for cosine similarity
        faiss.normalize_L2(embeddings)
        
        # Get starting index
        start_idx = len(self.chunks)
        
        # Add embeddings to FAISS index
        self.index.add(embeddings)
        
        # Store chunks with metadata
        for i, chunk_text in enumerate(chunks):
            self.chunks.append({
                "id": start_idx + i,
                "file_id": file_id,
                "content": chunk_text,
                "chunk_index": i,
                "tokens": len(chunk_text.split())  # Rough estimate
            })
        
        # Update metadata
        self.metadata["documents"][file_id] = {
            "filename": filename,
            "upload_date": datetime.utcnow().isoformat(),
            "num_chunks": len(chunks),
            "total_tokens": sum(len(c.split()) for c in chunks)
        }
        
        # Persist to disk
        self._save_all()
        
        logger.info(f"Added {len(chunks)} chunks for file {file_id}")
    
    def search(
        self,
        query_embedding: np.ndarray,
        top_k: int = 3,
        file_ids: Optional[List[str]] = None
    ) -> List[Dict]:
        """
        Search for similar chunks using HNSW
        
        Returns:
            List of dicts with keys: content, score, file_id, chunk_index
        """
        
        if self.index.ntotal == 0:
            logger.warning("Vector store is empty")
            return []
        
        # Ensure correct shape and type
        query_embedding = query_embedding.reshape(1, -1).astype('float32')
        
        # Normalize for cosine similarity
        faiss.normalize_L2(query_embedding)
        
        # Set search parameter
        self.index.hnsw.efSearch = settings.HNSW_EF_SEARCH
        
        # Perform search
        # Get more results if filtering by file_id
        k = top_k * 10 if file_ids else top_k
        k = min(k, self.index.ntotal)
        
        distances, indices = self.index.search(query_embedding, k)
        
        # Build results
        results = []
        for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):
            if idx == -1:  # FAISS returns -1 for missing results
                continue
            
            chunk = self.chunks[idx]
            
            # Filter by file_ids if specified
            if file_ids and chunk["file_id"] not in file_ids:
                continue
            
            # Convert distance to similarity score (cosine similarity)
            # L2 distance after normalization: d = 2 - 2*cos(theta)
            # So: cos(theta) = 1 - d/2
            similarity = 1 - (dist / 2)
            
            results.append({
                "content": chunk["content"],
                "score": float(similarity),
                "file_id": chunk["file_id"],
                "chunk_index": chunk["chunk_index"]
            })
            
            if len(results) >= top_k:
                break
        
        logger.info(f"Found {len(results)} similar chunks")
        return results
    
    def get_document_status(self, file_id: str) -> Dict:
        """Get processing status for a document"""
        
        if file_id in self.metadata["documents"]:
            doc = self.metadata["documents"][file_id]
            return {
                "status": "completed",
                "chunks_count": doc["num_chunks"],
                "upload_date": doc["upload_date"]
            }
        else:
            return {
                "status": "not_found",
                "chunks_count": 0
            }
    
    def delete_document(self, file_id: str):
        """Delete all chunks for a document"""
        
        # FAISS doesn't support deletion natively
        # We need to rebuild the index
        
        # Filter out chunks
        remaining_chunks = [c for c in self.chunks if c["file_id"] != file_id]
        
        if len(remaining_chunks) == len(self.chunks):
            logger.warning(f"No chunks found for file {file_id}")
            return
        
        # Remove from metadata
        if file_id in self.metadata["documents"]:
            del self.metadata["documents"][file_id]
        
        # Rebuild index
        logger.info(f"Rebuilding index after deleting {file_id}")
        self._rebuild_index(remaining_chunks)
    
    def _rebuild_index(self, chunks: List[Dict]):
        """Rebuild FAISS index from scratch"""
        
        # Create new index
        new_index = faiss.IndexHNSWFlat(self.dimension, settings.HNSW_M)
        new_index.hnsw.efConstruction = settings.HNSW_EF_CONSTRUCTION
        
        # Re-add embeddings (need to re-embed - this is a limitation)
        # For now, we'll keep it simple and require re-embedding
        # In production, you might want to store embeddings separately
        
        self.index = new_index
        self.chunks = chunks
        self._save_all()
    
    def _save_all(self):
        """Persist index and data to disk"""
        
        # Save FAISS index
        faiss.write_index(self.index, self.index_path)
        
        # Save chunks
        with open(self.chunks_path, 'w') as f:
            json.dump({"chunks": self.chunks}, f, indent=2)
        
        # Save metadata
        with open(self.metadata_path, 'w') as f:
            json.dump(self.metadata, f, indent=2)
        
        logger.info("Vector store saved to disk")
    
    def _load_chunks(self):
        """Load chunks from disk"""
        
        if os.path.exists(self.chunks_path):
            with open(self.chunks_path, 'r') as f:
                data = json.load(f)
                self.chunks = data.get("chunks", [])
    
    def _load_metadata(self):
        """Load metadata from disk"""
        
        if os.path.exists(self.metadata_path):
            with open(self.metadata_path, 'r') as f:
                self.metadata = json.load(f)
        else:
            self.metadata = {"documents": {}}

# Global vector store instance
_vector_store: Optional[VectorStore] = None

def get_vector_store() -> VectorStore:
    """Get or create vector store singleton"""
    global _vector_store
    if _vector_store is None:
        _vector_store = VectorStore()
    return _vector_store
```

---

## ðŸ” HNSW Configuration

### Parameters Explained

| Parameter | Value | Description |
|-----------|-------|-------------|
| `M` | 16 | Number of bidirectional links per node in each layer. Higher = more memory, better recall |
| `efConstruction` | 200 | Size of dynamic candidate list during construction. Higher = slower build, better quality |
| `efSearch` | 50 | Size of dynamic candidate list during search. Higher = slower search, better recall |

### Tuning Recommendations

**For Development (Current Settings)**:
- M=16, efConstruction=200, efSearch=50
- Good balance of speed and accuracy
- Suitable for datasets up to 100k vectors

**For Production (If needed)**:
- M=32, efConstruction=400, efSearch=100
- Better recall at cost of memory and speed
- For critical applications requiring high accuracy

---

## ðŸ“Š Performance Characteristics

### Search Speed
- **10k vectors**: ~1ms per query
- **100k vectors**: ~5ms per query
- **1M vectors**: ~20ms per query

### Memory Usage
- **Base**: `dimension * 4 bytes * num_vectors`
- **HNSW overhead**: `~M * 4 bytes * num_vectors`
- **Example**: 10k vectors of 1536 dims
  - Base: 10k * 1536 * 4 = 61MB
  - HNSW (M=16): 10k * 16 * 4 = 640KB
  - **Total**: ~62MB

### Disk Storage
- Index size approximately equals memory usage
- Chunks JSON: ~500 bytes per chunk average
- Metadata JSON: negligible

---

## ðŸ§ª Testing Vector Store

### `test_vector_store.py` (to be created during testing phase)

```python
import numpy as np
from services.vector_store import get_vector_store

def test_vector_store():
    """Test vector store operations"""
    
    vs = get_vector_store()
    
    # Create test embeddings
    embeddings = np.random.randn(5, 1536).astype('float32')
    chunks = [f"Test chunk {i}" for i in range(5)]
    
    # Add chunks
    vs.add_chunks(
        file_id="test-doc-1",
        filename="test.txt",
        chunks=chunks,
        embeddings=embeddings
    )
    
    # Search
    query_embedding = np.random.randn(1536).astype('float32')
    results = vs.search(query_embedding, top_k=3)
    
    print(f"Found {len(results)} results")
    for r in results:
        print(f"  Score: {r['score']:.3f} - {r['content'][:50]}")
    
    # Check status
    status = vs.get_document_status("test-doc-1")
    print(f"Document status: {status}")

if __name__ == "__main__":
    test_vector_store()
```

---

## ðŸ”„ Migration Path to Qdrant (Future)

If scaling beyond FAISS limitations, migration to Qdrant:

### Qdrant Advantages
1. Native HTTP API (no Python process needed)
2. Built-in filtering and metadata
3. Distributed deployment
4. No rebuild needed for deletions
5. Better multi-tenancy support

### Migration Strategy
```python
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams

# Create Qdrant collection with HNSW
client = QdrantClient("localhost", port=6333)
client.create_collection(
    collection_name="cosmic_ai",
    vectors_config=VectorParams(
        size=1536,
        distance=Distance.COSINE,
        hnsw_config={
            "m": 16,
            "ef_construct": 200
        }
    )
)
```

---

## âœ… Verification Steps

1. **Test Index Creation**
   ```python
   from services.vector_store import get_vector_store
   vs = get_vector_store()
   print(f"Index initialized: {vs.index is not None}")
   ```

2. **Test Adding Vectors**
   ```python
   import numpy as np
   emb = np.random.randn(3, 1536).astype('float32')
   chunks = ["test1", "test2", "test3"]
   vs.add_chunks("test-id", "test.txt", chunks, emb)
   print(f"Total vectors: {vs.index.ntotal}")
   ```

3. **Test Search**
   ```python
   query = np.random.randn(1536).astype('float32')
   results = vs.search(query, top_k=2)
   print(f"Found {len(results)} results")
   ```

4. **Test Persistence**
   ```python
   # Restart Python, reload
   vs2 = get_vector_store()
   print(f"Reloaded vectors: {vs2.index.ntotal}")
   ```

---

## ðŸ“ Dependencies

Add to `requirements.txt`:
```
faiss-cpu==1.9.0
numpy>=1.24.0
```

For GPU support (optional):
```
faiss-gpu==1.9.0
```

---

## ðŸš¨ Important Notes

1. **Thread Safety**: FAISS is not thread-safe. Use locks if accessing from multiple threads
2. **Normalization**: Always normalize embeddings for cosine similarity
3. **Dimension**: Must match Azure OpenAI embedding model (1536 for ada-002)
4. **Deletion**: FAISS doesn't support native deletion - requires index rebuild
5. **Backup**: Regularly backup `vector_store/` directory

---

## ðŸ“Œ Next Steps

1. Implement embedding service â†’ `03_Document_Processing.md`
2. Integrate vector store in document processing pipeline
3. Use search in chat endpoint
4. Add monitoring and metrics
